{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up local\n",
    "from enum import Enum\n",
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class LayoutItemLabel(str, Enum):\n",
    "\n",
    "    # Surya\n",
    "    BLANK = 'Blank'\n",
    "    TEXT = 'Text'\n",
    "    TEXTINLINEMATH = 'TextInlineMath'\n",
    "    CODE = 'Code'\n",
    "    SECTIONHEADER = 'SectionHeader'\n",
    "    CAPTION = 'Caption'\n",
    "    FOOTNOTE = 'Footnote'\n",
    "    EQUATION = 'Equation'\n",
    "    LISTITEM = 'ListItem'\n",
    "    PAGEFOOTER = 'PageFooter'\n",
    "    PAGEHEADER = 'PageHeader'\n",
    "    PICTURE = 'Picture'\n",
    "    FIGURE = 'Figure'\n",
    "    TABLE = 'Table'\n",
    "    FORM = 'Form'\n",
    "    TABLEOFCONTENTS = 'TableOfContents'\n",
    "    HANDWRITING = 'Handwriting'\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "\n",
    "class Cell(BaseModel):\n",
    "    id: int\n",
    "    text: Optional[str]\n",
    "    bbox: tuple[float | int, float | int, float | int, float | int]\n",
    "\n",
    "\n",
    "class ParsedPage(BaseModel):\n",
    "    page_no: int\n",
    "    cells: list[Cell]\n",
    "    size: Optional[tuple[float, float]] = None\n",
    "\n",
    "\n",
    "class Cluster(BaseModel):\n",
    "    id: int\n",
    "    label: LayoutItemLabel\n",
    "    bbox: list[float]\n",
    "    confidence: float = 1.0\n",
    "    cells: list[Cell] = []\n",
    "    content_text: Optional[str] = None\n",
    "    content_raw: Optional[str] = None  # image and table is base64\n",
    "    reading_order: str\n",
    "    page_no: int\n",
    "    next_id: Optional[int] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# Set up inputs\n",
    "import json\n",
    "\n",
    "with open(\"concat_clusters_output_ga.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "clusters: list[Cluster]= []\n",
    "for cluster in data[\"clusters\"]:\n",
    "    current_cluster = Cluster(\n",
    "        id=cluster[\"id\"],\n",
    "        label=cluster[\"label\"],\n",
    "        bbox=cluster[\"bbox\"],\n",
    "        confidence=cluster[\"confidence\"],\n",
    "        cells=cluster[\"cells\"],\n",
    "        content_text=cluster[\"content_text\"],\n",
    "        content_raw=cluster[\"content_raw\"],\n",
    "        reading_order=cluster[\"reading_order\"],\n",
    "        page_no=cluster[\"page_no\"],\n",
    "        next_id=cluster[\"next_id\"],\n",
    "    )\n",
    "    clusters.append(current_cluster)\n",
    "\n",
    "print(len(data[\"clusters\"]))\n",
    "print(len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "## Convert from cluster to chunk\n",
    "list_chunks: list[str] = []\n",
    "for cluster in clusters:\n",
    "    list_chunks.append(cluster.content_text)\n",
    "print(len(list_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build simple sematic split\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    SemanticSplitterNodeParser,\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding()\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model\n",
    ")\n",
    "\n",
    "# also baseline splitter\n",
    "base_splitter = SentenceSplitter(chunk_size=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Sequence, TypedDict\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "import numpy as np\n",
    "from llama_index.core.base.embeddings.base import BaseEmbedding\n",
    "from llama_index.core.bridge.pydantic import Field, SerializeAsAny\n",
    "from llama_index.core.callbacks.base import CallbackManager\n",
    "from llama_index.core.node_parser import NodeParser\n",
    "from llama_index.core.node_parser.node_utils import build_nodes_from_splits, default_id_func\n",
    "from llama_index.core.schema import BaseNode, Document\n",
    "\n",
    "class ChunkCombination(TypedDict):\n",
    "    chunk: str\n",
    "    index: int\n",
    "    combined_chunk: str\n",
    "    combined_chunk_embedding: List[float]\n",
    "\n",
    "class ChunkSemanticSplitterNodeParser(NodeParser):\n",
    "    \"\"\"Semantic node parser that works with pre-split chunks instead of sentences.\n",
    "    \n",
    "    Args:\n",
    "        buffer_size (int): number of chunks to group together when evaluating semantic similarity\n",
    "        embed_model (BaseEmbedding): embedding model to use\n",
    "        include_metadata (bool): whether to include metadata in nodes\n",
    "        include_prev_next_rel (bool): whether to include prev/next relationships\n",
    "    \"\"\"\n",
    "\n",
    "    embed_model: SerializeAsAny[BaseEmbedding] = Field(\n",
    "        description=\"The embedding model to use for semantic comparison\",\n",
    "    )\n",
    "\n",
    "    buffer_size: int = Field(\n",
    "        default=1,\n",
    "        description=(\n",
    "            \"The number of chunks to group together when evaluating semantic similarity. \"\n",
    "            \"Set to 1 to consider each chunk individually. \"\n",
    "            \"Set to >1 to group chunks together.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    breakpoint_percentile_threshold: int = Field(\n",
    "        default=95,\n",
    "        description=(\n",
    "            \"The percentile of cosine dissimilarity that must be exceeded between a \"\n",
    "            \"group of chunks and the next to form a node. The smaller this \"\n",
    "            \"number is, the more nodes will be generated\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def _parse_nodes(\n",
    "        self,\n",
    "        nodes: Sequence[BaseNode],\n",
    "        show_progress: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Implementation of abstract method from NodeParser.\"\"\"\n",
    "        all_nodes: List[BaseNode] = []\n",
    "        for node in nodes:\n",
    "            # Convert each node's text into chunks and process them\n",
    "            chunks = [node.text]  # Or implement your own chunking logic here\n",
    "            processed_nodes = self.parse_chunks(chunks, show_progress)\n",
    "            all_nodes.extend(processed_nodes)\n",
    "        return all_nodes\n",
    "\n",
    "    @classmethod\n",
    "    def from_defaults(\n",
    "        cls,\n",
    "        embed_model: Optional[BaseEmbedding] = None,\n",
    "        breakpoint_percentile_threshold: Optional[int] = 95,\n",
    "        buffer_size: Optional[int] = 1,\n",
    "        include_metadata: bool = True,\n",
    "        include_prev_next_rel: bool = True,\n",
    "        callback_manager: Optional[CallbackManager] = None,\n",
    "        id_func: Optional[callable] = None,\n",
    "    ) -> \"ChunkSemanticSplitterNodeParser\":\n",
    "        callback_manager = callback_manager or CallbackManager([])\n",
    "\n",
    "        if embed_model is None:\n",
    "            try:\n",
    "                from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "                embed_model = OpenAIEmbedding()\n",
    "            except ImportError:\n",
    "                raise ImportError(\n",
    "                    \"`llama-index-embeddings-openai` package not found, \"\n",
    "                    \"please run `pip install llama-index-embeddings-openai`\"\n",
    "                )\n",
    "\n",
    "        id_func = id_func or default_id_func\n",
    "\n",
    "        return cls(\n",
    "            embed_model=embed_model,\n",
    "            breakpoint_percentile_threshold=breakpoint_percentile_threshold,\n",
    "            buffer_size=buffer_size,\n",
    "            include_metadata=include_metadata,\n",
    "            include_prev_next_rel=include_prev_next_rel,\n",
    "            callback_manager=callback_manager,\n",
    "            id_func=id_func,\n",
    "        )\n",
    "\n",
    "    def parse_chunks(\n",
    "        self,\n",
    "        chunks: List[str],\n",
    "        show_progress: bool = False,\n",
    "    ) -> List[BaseNode]:\n",
    "        \"\"\"Parse pre-split chunks into semantically related nodes.\"\"\"\n",
    "        # Create a dummy document to hold the chunks\n",
    "        doc = Document(text=\"\".join(chunks))\n",
    "        \n",
    "        chunk_groups = self._build_chunk_groups(chunks)\n",
    "        \n",
    "        # Get embeddings for chunk groups\n",
    "        combined_chunk_embeddings = self.embed_model.get_text_embedding_batch(\n",
    "            [g[\"combined_chunk\"] for g in chunk_groups],\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "        \n",
    "        for i, embedding in enumerate(combined_chunk_embeddings):\n",
    "            chunk_groups[i][\"combined_chunk_embedding\"] = embedding\n",
    "\n",
    "        # Calculate semantic distances between groups\n",
    "        distances = self._calculate_distances_between_chunk_groups(chunk_groups)\n",
    "        \n",
    "        # Build final chunks based on semantic similarity\n",
    "        final_chunks = self._build_node_chunks(chunk_groups, distances)\n",
    "        \n",
    "        # Create nodes from the chunks\n",
    "        nodes = build_nodes_from_splits(\n",
    "            final_chunks,\n",
    "            doc,\n",
    "            id_func=self.id_func,\n",
    "        )\n",
    "        \n",
    "        return nodes\n",
    "\n",
    "    def _build_chunk_groups(self, chunks: List[str]) -> List[ChunkCombination]:\n",
    "        \"\"\"Build groups of chunks based on buffer size.\"\"\"\n",
    "        chunk_groups: List[ChunkCombination] = [\n",
    "            {\n",
    "                \"chunk\": x,\n",
    "                \"index\": i,\n",
    "                \"combined_chunk\": \"\",\n",
    "                \"combined_chunk_embedding\": [],\n",
    "            }\n",
    "            for i, x in enumerate(chunks)\n",
    "        ]\n",
    "\n",
    "        # Group chunks together based on buffer size\n",
    "        for i in range(len(chunk_groups)):\n",
    "            combined_chunk = \"\"\n",
    "            \n",
    "            # Add previous chunks based on buffer size\n",
    "            for j in range(i - self.buffer_size, i):\n",
    "                if j >= 0:\n",
    "                    combined_chunk += chunk_groups[j][\"chunk\"] + \" \"\n",
    "            \n",
    "            # Add current chunk\n",
    "            combined_chunk += chunk_groups[i][\"chunk\"]\n",
    "            \n",
    "            # Add next chunks based on buffer size\n",
    "            for j in range(i + 1, i + 1 + self.buffer_size):\n",
    "                if j < len(chunk_groups):\n",
    "                    combined_chunk += \" \" + chunk_groups[j][\"chunk\"]\n",
    "            \n",
    "            chunk_groups[i][\"combined_chunk\"] = combined_chunk.strip()\n",
    "\n",
    "        return chunk_groups\n",
    "\n",
    "    def _calculate_distances_between_chunk_groups(\n",
    "        self, chunk_groups: List[ChunkCombination]\n",
    "    ) -> List[float]:\n",
    "        \"\"\"Calculate semantic distances between consecutive chunk groups.\"\"\"\n",
    "        distances = []\n",
    "        for i in range(len(chunk_groups) - 1):\n",
    "            embedding_current = chunk_groups[i][\"combined_chunk_embedding\"]\n",
    "            embedding_next = chunk_groups[i + 1][\"combined_chunk_embedding\"]\n",
    "            \n",
    "            similarity = self.embed_model.similarity(embedding_current, embedding_next)\n",
    "            distance = 1 - similarity\n",
    "            \n",
    "            distances.append(distance)\n",
    "            \n",
    "        return distances\n",
    "\n",
    "    def _build_node_chunks(\n",
    "        self, chunk_groups: List[ChunkCombination], distances: List[float]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Build final chunks based on semantic similarity breakpoints.\"\"\"\n",
    "        final_chunks = []\n",
    "        \n",
    "        if len(distances) > 0:\n",
    "            breakpoint_distance_threshold = np.percentile(\n",
    "                distances, self.breakpoint_percentile_threshold\n",
    "            )\n",
    "\n",
    "            indices_above_threshold = [\n",
    "                i for i, x in enumerate(distances) if x > breakpoint_distance_threshold\n",
    "            ]\n",
    "\n",
    "            # Group chunks based on semantic breakpoints\n",
    "            start_index = 0\n",
    "            for index in indices_above_threshold:\n",
    "                group = chunk_groups[start_index:index + 1]\n",
    "                combined_text = \" \".join([g[\"chunk\"] for g in group])\n",
    "                final_chunks.append(combined_text)\n",
    "                start_index = index + 1\n",
    "\n",
    "            # Add remaining chunks\n",
    "            if start_index < len(chunk_groups):\n",
    "                combined_text = \" \".join(\n",
    "                    [g[\"chunk\"] for g in chunk_groups[start_index:]]\n",
    "                )\n",
    "                final_chunks.append(combined_text)\n",
    "        else:\n",
    "            # If only one chunk or no distances calculated, return the original chunk\n",
    "            final_chunks = [\" \".join([g[\"chunk\"] for g in chunk_groups])]\n",
    "\n",
    "        return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parser\n",
    "parser = ChunkSemanticSplitterNodeParser.from_defaults(\n",
    "    embed_model=embed_model,\n",
    "    buffer_size=1,  \n",
    "    breakpoint_percentile_threshold=95 \n",
    ")\n",
    "\n",
    "# Parse chunks into semantic nodes\n",
    "nodes = parser.parse_chunks(list_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Node 0\n",
      "I. NỘI DUNG QUẢN LÝ THIẾT BỊ 1. Người phụ trách 1.1. Văn phòng Hà Nội:\n",
      "+ Thiết bị làm việc:\n",
      "Ms. Trần Phương Thảo\n",
      "Ms. Nguyễn Thị Thùy Linh C\n",
      "+ Thiết bị văn phòng:\n",
      "Ms. Nguyễn Thị Thanh Hoa B\n",
      "1.2. Văn phòng Đà Nẵng: Ms. Hồ Thị Thu Hiền B\n",
      "Ms. Lê Thị Đăng Phúc\n",
      "1.3. Văn phòng Hồ Chí Minh:\n",
      "Ms. Huỳnh Ngọc Thảo Vy 2. Tài liệu liên quan:\n",
      "\n",
      "Value Node 1\n",
      "Hệ thống ASSET quản lý thiết bị: https://asset.sun-asterisk.vn/ Hướng dẫn sử dụng ASSET: [S_Asset] Hướng dẫn sử dụng cho FCOV Manager và FCOV Staff.pdf Quy định về việc sử dụng tài sản và xử lý mất hỏng tài sản công ty: [SHN_2024] QUY ĐỊNH VỀ VIỆC SỬ DỤNG TÀI SẢN VÀ XỬ LÝ MẤT, HỎNG T… Tiêu chuẩn cấp phát thiết bị: [SHN_2024] TIÊU CHUẨN CẤP THIẾT BỊ 2024 .docx 3.Quy trình cấp phát, thu hồi thiết bị: 3.1 Quy trình cấp phát thiết bị: - Nhân viên mới: GA cấp phát máy theo Tiêu chuẩn thiết bị vào ngày onboard - Nhân viên nghỉ sinh/ không lương comeback: Nhân viên tạo request, GA cấp máy theo tiêu chuẩn/ dựa vào request - Đối với trường hợp cần sử dụng thêm thiết bị để phục vụ công việc: ● Nhân viên có yêu cầu cấp phát thiết bị, tài sản phục vụ công việc, cần tạo yêu cầu thiết bị, tài sản trên hệ thống ASSET. Yêu cầu sau khi tạo sẽ được chuyển tiếp lên các cấp quản lý. ● Sau khi được Trưởng ban và Trưởng phòng phê duyệt, yêu cầu sẽ được chuyển tới FCOV Unit (Người phụ trách, Trưởng phòng Hành chính, Trưởng ban Vận hành hoạt động nền tảng doanh nghiệp). Trưởng ban Vận hành hoạt động nền tảng doanh nghiệp (Ms. NgocNTM) ở bước phê duyệt cuối sẽ xem xét và chuyển các yêu cầu đó tới người phụ trách tương ứng. Sau khi yêu cầu được Ms. NgocNTM xác nhận, người phụ trách sẽ tiến hành xử lý cấp phát ● Nhân viên nhận thiết bị và ký nhận tại khu vực làm việc của người phụ trách quản lý thiết bị 3.2. Quy trình thu hồi thiết bị: - Nhân viên nghỉ việc/ thai sản/ không lương bàn giao toàn bộ thiết bị làm việc, thẻ xe, thẻ nhân viên muộn nhất vào ngày làm việc cuối cùng tại Sun* - Trường hợp nghỉ thai sản/ không lương muốn giữ lại thiết bị phải tạo Request trên Asset và được PSM/UM xác nhận - Với những thiết bị nhân viên không có nhu cầu sử dụng sẽ trả thiết bị và ký trả trực tiếp tại khu vực làm việc của người phụ trách quản lý thiết bị II. NỘI DUNG ĐĂNG KÝ VĂN PHÒNG PHẨM (VPP)\n",
      "\n",
      "Value Node 2\n",
      "1. Người phụ trách: 1.1 Văn phòng Hà Nội\n",
      "Võ Nguyệt Anh (vo.nguyet.anh@sun-asterisk.com)\n",
      "1.2 Văn phòng Đà Nẵng\n",
      "Nguyễn Thị Thiên Ân B (nguyen.thi.thien.an-b@sun-asterisk.com)\n",
      "1.3 Văn phòng Hồ Chí Minh\n",
      "Huỳnh Ngọc Thảo Vy (huynh.ngoc.thao.vy@sun-asterisk.com)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, node in enumerate(nodes):\n",
    "    print(f\"Value Node {i}\")\n",
    "    print(str(node.text) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using with LLMs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test prompt\n",
    "import json \n",
    "\n",
    "\n",
    "def get_prompt(input):\n",
    "    OUTPUT_FORMAT = {\n",
    "        \"output\": [\n",
    "            {\n",
    "            \"start_chunk_id\": 1,\n",
    "            \"end_chunk_id\": 4},\n",
    "            {\n",
    "            \"start_chunk_id\": 5,\n",
    "            \"end_chunk_id\": 9},\n",
    "            {\n",
    "            \"start_chunk_id\": 10,\n",
    "            \"end_chunk_id\": 16}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    INPUT_EXAMPLE_FORMAT =[\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"text\": \"Nguyễn Văn A đã ra mắt từ ngày 1/1/2000 và đang làm việc tại Sun*.\",\n",
    "            \"layout\": \"content\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"text\": \"Trinh Văn A đã làm việc tại Sun*.\",\n",
    "            \"layout\": \"content\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    SYSTEM_MESSAGE = \"\"\"\n",
    "    # 1. Instruction\n",
    "    You are an expert in analyzing document\n",
    "    Given a list of text chunk, your mission is concatenate or split list of these text chunk to create list of meaningful paragraph \n",
    "    \"\"\"\n",
    "\n",
    "    USER_MESSAGE = \"\"\"\n",
    "    # 2. Required rules\n",
    "    - Given a list of text chunk for example: {ex_input}. \n",
    "    - Your mission is to split these list of text chunk into multiple text chunks (we will call it as cluster). Text combined in each cluster have a meaningful and compleled text content. \n",
    "    - You can split based on meaning of section content or title layout. For example title text chunk can be the first text chunk of cluster or list text chunk point to same meaning or content will be in one cluster.\n",
    "    - Your output is specify list of start and end chunk id for each cluster.\n",
    "    - You must ensure when combine all text in text chunks in one cluster (join with new line character), each cluster have a meaningful and compleled content, each cluster has at least 4 text chunks and 300 tokens and has max token is 600 tokens\n",
    "    - Chunk id in output must not duplicated. The start chunk id of cluster is the adjacent number of end chunker id of its previous cluster. for example: end_chunk_id =4, and the next start chunk id = 5 \n",
    "    \n",
    "    # 3. Input:\n",
    "    {input_json}\n",
    "\n",
    "    # 4. Output format: JSON\n",
    "    {output_format}\n",
    "    \n",
    "    # 5. Explain input:\n",
    "    - id: id of the text chunk, and it is also reading order of this chunk. Reading order start from low to high\n",
    "    - text: text content of this chunk\n",
    "    - layout: layout information of this chunk.\n",
    "    \"\"\"\n",
    "    \n",
    "    return SYSTEM_MESSAGE, USER_MESSAGE.format(output_format=OUTPUT_FORMAT, ex_input=INPUT_EXAMPLE_FORMAT, input_json=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_llm: list[dict] = []\n",
    "for cluster in clusters:\n",
    "    clusters_llm.append(\n",
    "        {\n",
    "            \"id\": cluster.id,\n",
    "            \"text\": cluster.content_text,\n",
    "            \"layout\": cluster.label\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt, user_prompt = get_prompt(clusters_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"output\": [\n",
      "        {\"start_chunk_id\": 0, \"end_chunk_id\": 4},\n",
      "        {\"start_chunk_id\": 5, \"end_chunk_id\": 10},\n",
      "        {\"start_chunk_id\": 11, \"end_chunk_id\": 18},\n",
      "        {\"start_chunk_id\": 19, \"end_chunk_id\": 23},\n",
      "        {\"start_chunk_id\": 24, \"end_chunk_id\": 25}\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
